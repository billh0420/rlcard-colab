{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gin-Rummy-DQNAgent.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhtEtQjiRqfI"
      },
      "source": [
        "pip install rlcard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rSz-FE8SBkd"
      },
      "source": [
        "pip install rlcard[torch]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJO13ODqT2F9"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkpz33iLSfye"
      },
      "source": [
        "import rlcard\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "from copy import deepcopy\n",
        "\n",
        "from datetime import datetime\n",
        "from pytz import timezone"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFuHUQIRSuou"
      },
      "source": [
        "from rlcard.agents.random_agent import RandomAgent\n",
        "from rlcard.models.gin_rummy_rule_models import GinRummyNoviceRuleAgent\n",
        "\n",
        "from rlcard.agents.dqn_agent import DQNAgent\n",
        "from rlcard.agents.dqn_agent import Estimator\n",
        "from rlcard.agents.dqn_agent import EstimatorNetwork\n",
        "from rlcard.agents.dqn_agent import Memory\n",
        "\n",
        "from rlcard.utils import reorganize\n",
        "from rlcard.utils import get_device\n",
        "from rlcard.utils import set_seed\n",
        "from rlcard.utils import Logger\n",
        "from rlcard.utils import tournament\n",
        "from rlcard.utils import plot_curve\n",
        "\n",
        "from rlcard.games.gin_rummy.game import GinRummyGame\n",
        "from rlcard.games.gin_rummy.player import GinRummyPlayer\n",
        "from rlcard.games.gin_rummy.utils.move import ScoreNorthMove, ScoreSouthMove, KnockMove, GinMove, DeclareDeadHandMove\n",
        "from rlcard.games.gin_rummy.utils.action_event import KnockAction, GinAction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s3KSERaVfRi"
      },
      "source": [
        "# Get current time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgCb3-HsVk86"
      },
      "source": [
        "def get_current_time():\n",
        "    return datetime.now(timezone('America/Chicago')).strftime('%I:%M:%S %p')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7fVCkpkUEjN"
      },
      "source": [
        "# Train Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OppW6ZrHTwiS"
      },
      "source": [
        "def save_model(agent, log_dir: str, model_name: str):\n",
        "    agent_path = os.path.join(log_dir, f'{model_name}.pth')\n",
        "    torch.save(agent, agent_path)\n",
        "    print('Model saved in', agent_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CVkpdULUPcq"
      },
      "source": [
        "def train_environment(env, seed, num_episodes, num_eval_games, evaluate_every, log_dir, model_name: str):\n",
        "    # Seed numpy, torch, random\n",
        "    set_seed(seed)\n",
        "\n",
        "    # Start training\n",
        "    algorithm = 'dqn'\n",
        "    agent = env.agents[0]\n",
        "    game_statistics = GameStatistics()\n",
        "    with Logger(log_dir) as logger:\n",
        "        for episode in range(num_episodes):\n",
        "            # Generate data from the environment\n",
        "            trajectories, payoffs = env.run(is_training=True)\n",
        "\n",
        "            # Reorgananize the data to be state, action, reward, next_state, done\n",
        "            trajectories = reorganize(trajectories, payoffs)\n",
        "\n",
        "            # Feed transitions into agent memory, and train the agent\n",
        "            # Here, we assume that DQN always plays the first position\n",
        "            # and the other players play randomly (if any)\n",
        "            for ts in trajectories[0]:\n",
        "                agent.feed(ts)\n",
        "\n",
        "            # Evaluate the performance. Play with random agents.\n",
        "            if episode % evaluate_every == 0:\n",
        "                logger.log(f\"\\n----------------------------------------\")\n",
        "                logger.log(f\"\\nEpisode: {episode}\")\n",
        "                logger.log_performance(env.timestep, tournament(env, num_eval_games)[0])\n",
        "            \n",
        "            # Save model?\n",
        "            if episode > 0 and episode % 100000 == 0:\n",
        "                save_model(agent=agent, log_dir=log_dir, model_name=model_name)\n",
        "\n",
        "            # Keep game statistics\n",
        "            game_statistics.update_statistics(game=env.game)\n",
        "\n",
        "    # Show game statistics\n",
        "    game_statistics.show_statistics()\n",
        "\n",
        "    # Get the paths\n",
        "    csv_path, fig_path = logger.csv_path, logger.fig_path\n",
        "\n",
        "    # Plot the learning curve\n",
        "    plot_curve(csv_path, fig_path, algorithm)\n",
        "\n",
        "    # Save model\n",
        "    save_model(agent=agent, log_dir=log_dir, model_name=model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXV59AqeUeos"
      },
      "source": [
        "# Get DQNAgent\n",
        "\n",
        "1. replay_memory_size = 200000 rather than 20K\n",
        "2. mlp_layers = [64, 64, 64] rather than [64, 64]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UEhG5_4UWTz"
      },
      "source": [
        "def get_dqn_agent(env, model_name: str, log_dir: str):\n",
        "    replay_memory_size = 200000\n",
        "    replay_memory_init_size = 1000 #100\n",
        "    update_target_estimator_every = 1000\n",
        "    discount_factor=0.99\n",
        "    epsilon_start=1.0\n",
        "    epsilon_end=0.1\n",
        "    epsilon_decay_steps=20000\n",
        "    batch_size=128 #32\n",
        "    train_every=100 #1\n",
        "    mlp_layers = [128, 128, 128]\n",
        "    learning_rate=0.00005 #0.00005\n",
        "    device = get_device()\n",
        "\n",
        "    agent_path = os.path.join(log_dir, f'{model_name}.pth')\n",
        "    if os.path.exists(agent_path):\n",
        "        train_id = 2 # increment for each train iteration\n",
        "        dqn_agent = torch.load(agent_path, map_location=device)\n",
        "        dqn_agent.set_device(device)\n",
        "        #dqn_agent.train_every = 100  # wch: note this\n",
        "        #dqn_agent.batch_size = 128  # wch: note this\n",
        "        #dqn_agent.memory.batch_size = dqn_agent.batch_size  # wch: note this\n",
        "        #dqn_agent.replay_memory_init_size = 1000  # wch: note this\n",
        "        #dqn_agent.q_estimator.optimizer.param_groups[0]['lr'] = 0.000005\n",
        "        #dqn_agent.target_estimator.optimizer.param_groups[0]['lr'] = 0.000005\n",
        "        print(f'train_id={train_id}; agent={dqn_agent}')\n",
        "        print(f'train_steps={dqn_agent.train_t} time_steps={dqn_agent.total_t}')\n",
        "    else:\n",
        "        train_id = 1 # reset for first train iteration\n",
        "        num_actions = env.game.get_num_actions()\n",
        "        state_shape = env.state_shape[0] # state_shape for player_id = 0\n",
        "        dqn_agent = DQNAgent(\n",
        "            replay_memory_size,\n",
        "            replay_memory_init_size,\n",
        "            update_target_estimator_every,\n",
        "            discount_factor,\n",
        "            epsilon_start,\n",
        "            epsilon_end,\n",
        "            epsilon_decay_steps,\n",
        "            batch_size,\n",
        "            num_actions,\n",
        "            state_shape,\n",
        "            train_every,\n",
        "            mlp_layers,\n",
        "            learning_rate,\n",
        "            device=device)\n",
        "    return dqn_agent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFmB6BAyW5ld"
      },
      "source": [
        "# Gin Rummy Scoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_ev9aQfW895"
      },
      "source": [
        "def get_payoff_gin_rummy(player: GinRummyPlayer, game: GinRummyGame) -> float:\n",
        "    ''' Get the payoff of player:\n",
        "            a) 1.0 if player gins\n",
        "            b) 1.0 if player knocks\n",
        "            c) 0.0 otherwise\n",
        "        The goal is to have the agent learn how to knock and gin.\n",
        "    Returns:\n",
        "        payoff (int or float): payoff for player (higher is better)\n",
        "    '''\n",
        "    going_out_action = game.round.going_out_action\n",
        "    going_out_player_id = game.round.going_out_player_id\n",
        "    if going_out_player_id == player.player_id and isinstance(going_out_action, KnockAction):\n",
        "        payoff = 1\n",
        "    elif going_out_player_id == player.player_id and isinstance(going_out_action, GinAction):\n",
        "        payoff = 1\n",
        "    else:\n",
        "        payoff = 0\n",
        "    return payoff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0jD0G12XQSw"
      },
      "source": [
        "# Game Statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJjpfs_vXSyK"
      },
      "source": [
        "class GameStatistics(object):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.game_count = 0\n",
        "        self.knock_count = 0\n",
        "        self.gin_count = 0\n",
        "        self.declare_dead_hand_count = 0\n",
        "        self.moves_per_game = 0\n",
        "\n",
        "    def update_statistics(self, game):\n",
        "        move_sheet = game.round.move_sheet\n",
        "        self.game_count += 1\n",
        "        if len(move_sheet) > 2:\n",
        "            going_out_move = move_sheet[-3]\n",
        "            if isinstance(going_out_move, KnockMove):\n",
        "                if going_out_move.player.player_id == 0:\n",
        "                    self.knock_count += 1\n",
        "            elif isinstance(going_out_move, GinMove):\n",
        "                if going_out_move.player.player_id == 0:\n",
        "                    self.gin_count += 1\n",
        "            elif isinstance(going_out_move, DeclareDeadHandMove):\n",
        "                self.declare_dead_hand_count += 1\n",
        "        self.moves_per_game += len(move_sheet)\n",
        "\n",
        "    def show_statistics(self):\n",
        "        print(f'game_count={self.game_count}')\n",
        "        print(f'knock_count={self.knock_count}')\n",
        "        print(f'gin_count={self.gin_count}')\n",
        "        print(f'declare_dead_hand_count={self.declare_dead_hand_count}')\n",
        "        print(f'average_moves_per_game={self.moves_per_game / self.game_count}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znkxS0HYU2-8"
      },
      "source": [
        "# Train DQNAgent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlHxV6vRUpqF"
      },
      "source": [
        "def train_dqn_agent(log_dir: str, model_name: str, multiplier: int, seed):\n",
        "    # Make the environment with seed\n",
        "    gin_rummy_env = rlcard.make('gin-rummy', config={'seed': seed})\n",
        "    num_actions = gin_rummy_env.game.get_num_actions()\n",
        "\n",
        "    # patch gin rummy game\n",
        "    gin_rummy_env.game.judge.scorer.get_payoff = get_payoff_gin_rummy # wch: Note this\n",
        "    gin_rummy_env.game.settings.is_always_knock = True # wch: Note this\n",
        "\n",
        "    # define opponent agents\n",
        "    ginRummyNoviceRuleAgent = GinRummyNoviceRuleAgent()\n",
        "    random_agent = RandomAgent(num_actions=num_actions)\n",
        "\n",
        "    # Initialize the agents for the environment\n",
        "    dqn_agent = get_dqn_agent(env=gin_rummy_env, model_name=model_name, log_dir=log_dir)\n",
        "    agents = [dqn_agent, ginRummyNoviceRuleAgent]\n",
        "    gin_rummy_env.set_agents(agents)\n",
        "\n",
        "    # print info\n",
        "    state_shape = gin_rummy_env.state_shape[0] # state_shape for player_id = 0\n",
        "    action_shape = gin_rummy_env.action_shape\n",
        "    print(f'log_dir={log_dir}')\n",
        "    print(f'model_name={model_name}')\n",
        "    print(f'state_shape={state_shape}')\n",
        "    print(f'action_shape={action_shape}')\n",
        "    print(f'num_actions={num_actions}')\n",
        "    print(dqn_agent.q_estimator.qnet)\n",
        "\n",
        "    #train the agent\n",
        "    num_episodes = 1000 * multiplier\n",
        "    evaluate_every = 20 * multiplier\n",
        "    num_eval_games = 200 #100\n",
        "    train_environment(gin_rummy_env,\n",
        "         seed=seed,\n",
        "         num_episodes=num_episodes,\n",
        "         num_eval_games=num_eval_games,\n",
        "         evaluate_every=evaluate_every,\n",
        "         log_dir=log_dir,\n",
        "         model_name=model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "invcWghmVBBs"
      },
      "source": [
        "my_log_dir='experiments/gin_rummy_dqn_result/'\n",
        "my_model_name='model_gin_rummy_dqnagent'\n",
        "my_multiplier = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hk5j9ux3VUOX"
      },
      "source": [
        "%%time\n",
        "print(f\"Start: {get_current_time()}\")\n",
        "if torch.cuda.is_available():\n",
        "    !nvidia-smi\n",
        "train_dqn_agent(log_dir=my_log_dir, model_name=my_model_name, multiplier=my_multiplier, seed=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkUxfWOJWXcb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}