{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gin-Rummy-DQNAgent.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1H1KrhJvFJO"
      },
      "source": [
        "# README\n",
        "\n",
        "* You should single step each cell to see what is going on.\n",
        "\n",
        "* The results are saved in: my_log_dir='colab_experiments/results_gin_rummy_dqn/'.\n",
        "\n",
        "    This is tempoary storage for your colab session: it will be deleted when you leave colab.\n",
        "\n",
        "* The model name is: my_model_name='model_gin_rummy_dqnagent'\n",
        "\n",
        "* You can rerun the cell with \"my_trainer.train(...)\" to continue training the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TS7Nd6ZL9y9"
      },
      "source": [
        "\n",
        "#Definitions\n",
        "* Transition = namedtuple('Transition', ['state', 'action', 'reward', 'next_state', 'next_legal_actions', 'done'])\n",
        "* Trajectory is a list of transitions made by an agent\n",
        "\n",
        "# Agent\n",
        "\n",
        "The Agent has these functions:\n",
        "* step(state) -> action_id\n",
        "* eval_step(state) -> action_id, probabilites\n",
        "* feed(trajectory)\n",
        "* predict(state) -> q_values\n",
        "\n",
        "If the agent is not trainable:\n",
        "* feed(trajectory) does nothing\n",
        "* predict(state) returns None\n",
        "\n",
        "# Environment\n",
        "\n",
        "The Environment holds information about:\n",
        "* game\n",
        "* agents\n",
        "* state_shape\n",
        "* action_shape\n",
        "* timestep (incremented every time any agent takes a step)\n",
        "\n",
        "The Environment has these functions:\n",
        "* set_agents(agents)\n",
        "* run(is_training) -> Trajectories, Payoffs\n",
        "\n",
        "The Environment determines:\n",
        "* how the state of the game is represented\n",
        "* what is the state_shape\n",
        "* what is the action_shape\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJO13ODqT2F9"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkpz33iLSfye"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "from copy import deepcopy\n",
        "from datetime import datetime\n",
        "from pytz import timezone"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGfjPPfZu4HZ"
      },
      "source": [
        "# Install RLCard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhtEtQjiRqfI"
      },
      "source": [
        "if not os.path.exists(os.path.join('/content/', 'rlcard')):\n",
        "  !git clone -b master --single-branch --depth=1 https://github.com/datamllab/rlcard.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLjQ9L-v8phP"
      },
      "source": [
        "# Change to top level rlcard directory if not already there"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH9WzLFkyTlQ"
      },
      "source": [
        "if not os.getcwd() == '/content/rlcard':\n",
        "  %cd /content/rlcard\n",
        "%pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npQQGQCN8Xcp"
      },
      "source": [
        "# Imports for RLCard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFuHUQIRSuou"
      },
      "source": [
        "import rlcard\n",
        "from rlcard.agents.random_agent import RandomAgent\n",
        "from rlcard.models.gin_rummy_rule_models import GinRummyNoviceRuleAgent\n",
        "\n",
        "from rlcard.agents.dqn_agent import DQNAgent\n",
        "from rlcard.agents.dqn_agent import Estimator\n",
        "from rlcard.agents.dqn_agent import EstimatorNetwork\n",
        "from rlcard.agents.dqn_agent import Memory\n",
        "\n",
        "from rlcard.utils import reorganize\n",
        "from rlcard.utils import get_device\n",
        "from rlcard.utils import set_seed\n",
        "from rlcard.utils import Logger\n",
        "from rlcard.utils import tournament\n",
        "from rlcard.utils import plot_curve\n",
        "\n",
        "from rlcard.envs.env import Env\n",
        "from rlcard.envs.gin_rummy import GinRummyEnv\n",
        "\n",
        "from rlcard.games.gin_rummy.game import GinRummyGame\n",
        "from rlcard.games.gin_rummy.player import GinRummyPlayer\n",
        "from rlcard.games.gin_rummy.utils.move import ScoreNorthMove, ScoreSouthMove, DrawCardMove, PickupDiscardMove, KnockMove, GinMove, DeclareDeadHandMove\n",
        "from rlcard.games.gin_rummy.utils.action_event import KnockAction, GinAction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFmB6BAyW5ld"
      },
      "source": [
        "# Gin Rummy Scoring function\n",
        "\n",
        "The get_gin_rummy_environment function uses this to set the payoffs for the Gin Rummy game.\n",
        "\n",
        "A player gets 1 point for going out (knock or gin) else 0 points (dead hand or lose).\n",
        "\n",
        "The DQNAgent needs to learn how to form and keep melds so that he can go out.\n",
        "\n",
        "The DQNAgent should pickup the discard when it would form a meld.\n",
        "Otherwise, he should draw a card (altough an exper player might break this rule sometimes)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_ev9aQfW895"
      },
      "source": [
        "def get_payoff_gin_rummy(player: GinRummyPlayer, game: GinRummyGame) -> float:\n",
        "    ''' Get the payoff of player:\n",
        "            a) 1.0 if player gins\n",
        "            b) 1.0 if player knocks\n",
        "            c) 0.0 otherwise\n",
        "        The goal is to have the agent learn how to knock and gin.\n",
        "    Returns:\n",
        "        payoff (int or float): payoff for player (higher is better)\n",
        "    '''\n",
        "    going_out_action = game.round.going_out_action\n",
        "    going_out_player_id = game.round.going_out_player_id\n",
        "    if going_out_player_id == player.player_id and isinstance(going_out_action, KnockAction):\n",
        "        payoff = 1\n",
        "    elif going_out_player_id == player.player_id and isinstance(going_out_action, GinAction):\n",
        "        payoff = 1\n",
        "    else:\n",
        "        payoff = 0\n",
        "    return payoff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5kn7UeXD0MB"
      },
      "source": [
        "# Get Gin Rummy Environment function\n",
        "\n",
        "Note that we patch the Gin Rummy game that is in the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aXfm3Mu4Gww"
      },
      "source": [
        "def get_gin_rummy_environment(seed) -> GinRummyEnv:\n",
        "    # Make the environment with seed\n",
        "    gin_rummy_env = rlcard.make('gin-rummy', config={'seed': seed})\n",
        "\n",
        "    # patch gin rummy game\n",
        "    gin_rummy_env.game.judge.scorer.get_payoff = get_payoff_gin_rummy # wch: Note this\n",
        "    gin_rummy_env.game.settings.is_always_knock = True # wch: Note this\n",
        "\n",
        "    # print info\n",
        "    state_shape = gin_rummy_env.state_shape[0] # state_shape for player_id = 0\n",
        "    action_shape = gin_rummy_env.action_shape\n",
        "    num_actions = gin_rummy_env.game.get_num_actions()\n",
        "    print(f'state_shape={state_shape}')\n",
        "    print(f'action_shape={action_shape}')\n",
        "    print(f'num_actions={num_actions}')\n",
        "    return gin_rummy_env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXV59AqeUeos"
      },
      "source": [
        "# Get DQNAgent function\n",
        "\n",
        "The train_dqn_agent function uses this to get the DQNAgent.\n",
        "\n",
        "The critical hyper parameters for the DQNAgent are:\n",
        "\n",
        "* replay_memory_size = 200000 rather than 20K\n",
        "* replay_memory_init_size = 1000 rather than 100\n",
        "* batch_size  =128 rather than 32\n",
        "* train_every = 100 rather than 1\n",
        "* mlp_layers = [128, 128, 128] rather than [64, 64]\n",
        "* learning_rate=0.00005 You might want to modify this.\n",
        "\n",
        "The DQNAgent has instance variables and three components:\n",
        "\n",
        "* Instance variables:\n",
        "    1. update_target_estimator_every\n",
        "    2. train_every\n",
        "    3. discount_factor\n",
        "    4. epsilon_start\n",
        "    5. epsilon_end\n",
        "    6. epsilon_decay_step\n",
        "\n",
        "* Estimator:\n",
        "    1. learning_rate\n",
        "    2. device\n",
        "\n",
        "* Estimator Network:\n",
        "    1. state_shape\n",
        "    2. mlp_layers\n",
        "    3. num_actions\n",
        "\n",
        "* Replay Memory:\n",
        "    1. replay_memory_size\n",
        "    2. replay_memory_init_size\n",
        "    3. batch_size\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UEhG5_4UWTz"
      },
      "source": [
        "def get_dqn_agent(env, model_name: str, log_dir: str):\n",
        "    replay_memory_size = 200000 # 20000\n",
        "    replay_memory_init_size = 1000 #100\n",
        "    update_target_estimator_every = 1000\n",
        "    discount_factor=0.99\n",
        "    epsilon_start=1.0\n",
        "    epsilon_end=0.1\n",
        "    epsilon_decay_steps=20000\n",
        "    batch_size=128 #32\n",
        "    train_every=100 #1\n",
        "    mlp_layers = [128, 128, 128] # [64, 64]\n",
        "    learning_rate=0.00005 #0.00005\n",
        "    device = get_device()\n",
        "\n",
        "    agent_path = os.path.join(log_dir, f'{model_name}.pth')\n",
        "    if os.path.exists(agent_path):\n",
        "        train_id = 2 # increment for each train iteration\n",
        "        dqn_agent = torch.load(agent_path, map_location=device)\n",
        "        dqn_agent.set_device(device)\n",
        "        #dqn_agent.train_every = 100  # wch: note this\n",
        "        #dqn_agent.batch_size = 128  # wch: note this\n",
        "        #dqn_agent.memory.batch_size = dqn_agent.batch_size  # wch: note this\n",
        "        #dqn_agent.replay_memory_init_size = 1000  # wch: note this\n",
        "        #dqn_agent.q_estimator.optimizer.param_groups[0]['lr'] = 0.000005\n",
        "        #dqn_agent.target_estimator.optimizer.param_groups[0]['lr'] = 0.000005\n",
        "        print(f'train_id={train_id}; agent={dqn_agent}')\n",
        "        print(f'train_steps={dqn_agent.train_t} time_steps={dqn_agent.total_t}')\n",
        "    else:\n",
        "        train_id = 1 # reset for first train iteration\n",
        "        num_actions = env.game.get_num_actions()\n",
        "        state_shape = env.state_shape[0] # state_shape for player_id = 0\n",
        "        dqn_agent = DQNAgent(\n",
        "            replay_memory_size,\n",
        "            replay_memory_init_size,\n",
        "            update_target_estimator_every,\n",
        "            discount_factor,\n",
        "            epsilon_start,\n",
        "            epsilon_end,\n",
        "            epsilon_decay_steps,\n",
        "            batch_size,\n",
        "            num_actions,\n",
        "            state_shape,\n",
        "            train_every,\n",
        "            mlp_layers,\n",
        "            learning_rate,\n",
        "            device=device)\n",
        "    print(f'log_dir={log_dir}')\n",
        "    print(f'model_name={model_name}')\n",
        "    print(dqn_agent.q_estimator.qnet)\n",
        "    return dqn_agent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0jD0G12XQSw"
      },
      "source": [
        "# Game Statistics class\n",
        "\n",
        "The Train class train function uses this to accumulate statistics for each completed game.\n",
        "\n",
        "The Train class train function uses this to show the accumulated statistics when the training session ends.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJjpfs_vXSyK"
      },
      "source": [
        "class GameStatistics(object):  # interface\n",
        "    def update_statistics(self, game):\n",
        "        pass\n",
        "\n",
        "    def show_statistics(self):\n",
        "        pass\n",
        "\n",
        "class GinRummyGameStatistics(GameStatistics):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.game_count = 0\n",
        "        self.north_draw_card_move_count = 0\n",
        "        self.north_pickup_discard_move_count = 0\n",
        "        self.knock_count = 0\n",
        "        self.gin_count = 0\n",
        "        self.declare_dead_hand_count = 0\n",
        "        self.moves_per_game = 0\n",
        "\n",
        "    def update_statistics(self, game):\n",
        "        move_sheet = game.round.move_sheet\n",
        "        self.game_count += 1\n",
        "        if len(move_sheet) > 2:\n",
        "            north_draw_card_moves = [move for move in move_sheet if isinstance(move, DrawCardMove) and move.player.player_id == 0]\n",
        "            self.north_draw_card_move_count += len(north_draw_card_moves)\n",
        "            north_pickup_discard_moves = [move for move in move_sheet if isinstance(move, PickupDiscardMove) and move.player.player_id == 0]\n",
        "            self.north_pickup_discard_move_count += len(north_pickup_discard_moves)\n",
        "            going_out_move = move_sheet[-3]\n",
        "            if isinstance(going_out_move, KnockMove):\n",
        "                if going_out_move.player.player_id == 0:\n",
        "                    self.knock_count += 1\n",
        "            elif isinstance(going_out_move, GinMove):\n",
        "                if going_out_move.player.player_id == 0:\n",
        "                    self.gin_count += 1\n",
        "            elif isinstance(going_out_move, DeclareDeadHandMove):\n",
        "                self.declare_dead_hand_count += 1\n",
        "        self.moves_per_game += len(move_sheet)\n",
        "\n",
        "    def show_statistics(self):\n",
        "        print(f'game_count={self.game_count}')\n",
        "        print(f'north_draw_card_moves_per_game={self.north_draw_card_move_count / self.game_count}')\n",
        "        print(f'north_pickup_discard_moves_per_game={self.north_pickup_discard_move_count / self.game_count}')\n",
        "        print(f'knock_count={self.knock_count}')\n",
        "        print(f'gin_count={self.gin_count}')\n",
        "        print(f'declare_dead_hand_count={self.declare_dead_hand_count}')\n",
        "        print(f'average_moves_per_game={self.moves_per_game / self.game_count}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znkxS0HYU2-8"
      },
      "source": [
        "# Trainer class\n",
        "\n",
        "This class is initialized with an environment.\n",
        "\n",
        "You must set the agents for the environment before you call the train method.\n",
        "\n",
        "The first agent must be the agent to be trained.\n",
        "\n",
        "=====================================\n",
        "\n",
        "In this notebook, the first agent is the DQNAgent and the second agent is the GinRummyNoviceRuleAgent.\n",
        "\n",
        "Training the DQNAgent against the RandomAgent is easy, but the trained DQNAgent learns a lot of bad habits. The play of the trained DQNAgent against the RandomAgent results in long games where both players frequently pick up the opponent's discard. The DQNAgent learns how to form melds (so that the game doesn't end as a dead hand), but waste times picking up the opponent's discard.\n",
        "\n",
        "====================================="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4IH20pdMB-d"
      },
      "source": [
        "class Trainer(object):\n",
        "\n",
        "    def __init__(self, log_dir: str, model_name: str, env: Env):\n",
        "        self.log_dir = log_dir\n",
        "        self.model_name = model_name\n",
        "        self.algorithm = 'dqn'\n",
        "        self.env = env\n",
        "        self.game_statistics = GameStatistics()\n",
        "    \n",
        "    def train(self, num_episodes: int, evaluate_every: int, seed):\n",
        "        with Logger(self.log_dir) as logger:\n",
        "            num_eval_games = 100 # 2000\n",
        "            # Seed numpy, torch, random\n",
        "            set_seed(seed)\n",
        "            for episode in range(num_episodes):\n",
        "                # Generate data from the environment\n",
        "                trajectories, payoffs = self.env.run(is_training=True)\n",
        "                # Reorganaize the data to be state, action, reward, next_state, done\n",
        "                trajectories = reorganize(trajectories, payoffs)\n",
        "\n",
        "                # Feed transitions into agent memory\n",
        "                self.feed(trajectories)\n",
        "\n",
        "                # Keep game statistics\n",
        "                self.game_statistics.update_statistics(game=self.env.game)\n",
        "\n",
        "                # Evaluate the performance.\n",
        "                if episode % evaluate_every == 0:\n",
        "                    self.evaluate_performance(episode=episode, evaluate_every=evaluate_every, num_eval_games=num_eval_games, logger=logger)\n",
        "                            \n",
        "                # Save model when number of episodes is very large ?\n",
        "                if episode > 0 and episode % 100000 == 0:\n",
        "                    self.save_model(agent=self.env.agents[0])\n",
        "\n",
        "        # Show game statistics\n",
        "        self.game_statistics.show_statistics()\n",
        "        # Plot the learning curve\n",
        "        self.plot_learning_curve(logger=logger)\n",
        "        # Save model\n",
        "        self.save_model(agent=self.env.agents[0])\n",
        "    \n",
        "    def feed(self, trajectories):\n",
        "        # Feed transitions into agent memory, and train the agent\n",
        "        # Here, we assume that DQN always plays the first position\n",
        "        # and the other players play randomly (if any)\n",
        "        for ts in trajectories[0]:\n",
        "            self.env.agents[0].feed(ts)\n",
        "\n",
        "    def evaluate_performance(self, episode: int, evaluate_every: int, num_eval_games: int, logger):\n",
        "        # Evaluate the performance.\n",
        "        logger.log(f\"\\n----------------------------------------\")\n",
        "        logger.log(f\"\\nEpisode: {episode}\")\n",
        "        logger.log_performance(self.env.timestep, tournament(self.env, num_eval_games)[0])\n",
        "    \n",
        "    def plot_learning_curve(self, logger):\n",
        "        # Plot the learning curve\n",
        "        csv_path, fig_path = logger.csv_path, logger.fig_path\n",
        "        plot_curve(csv_path, fig_path, self.algorithm)\n",
        "\n",
        "    def save_model(self, agent):\n",
        "        # Save model\n",
        "        # Note: Saving the agent is better than saving just the model.\n",
        "        save_path = os.path.join(self.log_dir, f'{self.model_name}.pth')\n",
        "        torch.save(agent, save_path)\n",
        "        print('Model saved in', save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s3KSERaVfRi"
      },
      "source": [
        "# Get current time function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgCb3-HsVk86"
      },
      "source": [
        "def get_current_time():\n",
        "    return datetime.now(timezone('America/Chicago')).strftime('%I:%M:%S %p')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPu5nSF94UzQ"
      },
      "source": [
        "# Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "invcWghmVBBs"
      },
      "source": [
        "my_log_dir='colab_experiments/results_gin_rummy_dqn/'\n",
        "my_model_name='model_gin_rummy_dqnagent'\n",
        "my_seed = None\n",
        "my_num_episodes = 100\n",
        "my_evaluate_every = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a4RzZpK4c9A"
      },
      "source": [
        "# Train DQNAgent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvVyEJekelZz"
      },
      "source": [
        "%%time\n",
        "print(f\"Start: {get_current_time()}\")\n",
        "if torch.cuda.is_available():\n",
        "    !nvidia-smi\n",
        "# define gin rummy environment\n",
        "my_gin_rummy_env = get_gin_rummy_environment(seed=my_seed)\n",
        "my_gin_rummy_env.game.settings.print_settings()\n",
        "# define learning agent\n",
        "my_learning_agent = get_dqn_agent(env=my_gin_rummy_env, model_name=my_model_name, log_dir=my_log_dir)\n",
        "# define opponent agent\n",
        "my_opponent_agent = GinRummyNoviceRuleAgent()\n",
        "# set agents for environment\n",
        "my_gin_rummy_env.set_agents([my_learning_agent, my_opponent_agent])\n",
        "# train dqn_agent\n",
        "my_trainer = Trainer(log_dir=my_log_dir, model_name=my_model_name, env=my_gin_rummy_env)\n",
        "my_trainer.game_statistics = GinRummyGameStatistics()\n",
        "my_trainer.train(num_episodes=my_num_episodes, evaluate_every=my_evaluate_every, seed=my_seed)\n",
        "print(f\"End: {get_current_time()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkUxfWOJWXcb"
      },
      "source": [
        "print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFAKtyozOoBv"
      },
      "source": [
        "# play_gin_rummy_tournament function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laUFkGdfOlbD"
      },
      "source": [
        "def play_gin_rummy_tournament_with_agents(north_agent, south_agent, num_games: int):\n",
        "    tournament_env = rlcard.make('gin-rummy', config={'seed': None})\n",
        "    tournament_env.game.judge.scorer.get_payoff = get_payoff_gin_rummy # wch: Note this\n",
        "    tournament_env.game.settings.is_always_knock = True # wch: Note this\n",
        "    tournament_env.set_agents([north_agent, south_agent])\n",
        "    payoffs = tournament(tournament_env, num_games)\n",
        "    print(f\"payoffs=[{payoffs[0]}, {payoffs[1]}]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pe5NEyJ3OyRa"
      },
      "source": [
        "def play_gin_rummy_tournament(log_dir, model_name, num_games):\n",
        "    agent_path = os.path.join(log_dir, f'{model_name}.pth')\n",
        "    device = get_device()\n",
        "    gin_rummy_dqn_agent = torch.load(agent_path, map_location=device)\n",
        "    gin_rummy_dqn_agent.set_device(device)\n",
        "    print(f\"gin_rummy_dqn_agent={gin_rummy_dqn_agent}\")\n",
        "    ginRummyNoviceRuleAgent = GinRummyNoviceRuleAgent()\n",
        "    num_actions = gin_rummy_dqn_agent.num_actions\n",
        "    random_agent = RandomAgent(num_actions=num_actions)\n",
        "    num_games_preliminary = 100\n",
        "    for tournament_id in range(5):\n",
        "        print(f\"----- preliminary tournament {tournament_id + 1} {num_games_preliminary} games ------\")\n",
        "        play_gin_rummy_tournament_with_agents(gin_rummy_dqn_agent, ginRummyNoviceRuleAgent, num_games = num_games_preliminary)\n",
        "        play_gin_rummy_tournament_with_agents(gin_rummy_dqn_agent, random_agent, num_games = num_games_preliminary)\n",
        "        play_gin_rummy_tournament_with_agents(ginRummyNoviceRuleAgent, random_agent, num_games = num_games_preliminary)\n",
        "    print(f\"----- final tournament gin_rummy_dqn_agent vs ginRummyNoviceRuleAgent {num_games} games  ------\")\n",
        "    play_gin_rummy_tournament_with_agents(gin_rummy_dqn_agent, ginRummyNoviceRuleAgent, num_games = num_games)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCUWSh_BO6o-"
      },
      "source": [
        "# Gin Rummy Tournaments\n",
        "\n",
        "There are 5 preliminary tournaments of 100 games per tournament:\n",
        "\n",
        "*   gin_rummy_dqn_agent vs ginRummyNoviceRuleAgent\n",
        "*   gin_rummy_dqn_agent vs randomAgent\n",
        "*   ginRummyNoviceRuleAgent vs randomAgent\n",
        "\n",
        "There is one final tournament of {num_games} games per tournament:\n",
        "\n",
        "*   gin_rummy_dqn_agent vs ginRummyNoviceRuleAgent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLIJD0OrPCOB"
      },
      "source": [
        "play_gin_rummy_tournament(log_dir=my_log_dir, model_name=my_model_name, num_games=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izsFS7v3PN5o"
      },
      "source": [
        "# Print local variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLs2SJ1mPaCl"
      },
      "source": [
        "for my_type in ['str', 'int', 'float', 'list', 'dict', 'NoneType']:\n",
        "    print(f'---------------------------------')\n",
        "    print(f'{my_type}')\n",
        "    %whos {my_type}\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwG0d2BzgD35"
      },
      "source": [
        "%whos str int list dict NoneType"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-HUvpgWvI9z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}