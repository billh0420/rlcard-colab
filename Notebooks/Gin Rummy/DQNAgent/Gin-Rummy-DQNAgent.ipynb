{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Gin-Rummy-DQNAgent.ipynb","provenance":[{"file_id":"https://github.com/billh0420/rlcard-colab/blob/main/Notebooks/Gin%20Rummy/DQNAgent/Gin_Rummy_DQNAgent.ipynb","timestamp":1638374477880}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"f1H1KrhJvFJO"},"source":["# README\n","\n","* You should single step each cell to see what is going on.\n","\n","* The results are saved in: my_log_dir='experiments/results_gin_rummy_dqn/'.\n","\n","    This is tempoary storage for your colab session: it will be deleted when you leave colab.\n","\n","* The model name is: my_model_name='model_gin_rummy_dqnagent'\n","\n","* You can rerun the cell with \"train_dqn_agent(...)\" to continue training the model."]},{"cell_type":"markdown","metadata":{"id":"mmu0C8m-Ait-"},"source":["# Optionally save work in your google drive to keep it after you logout\n","\n","If you wish to save work in your google drive:\n","* change my_working_directory to something of your choice\n","* change False to True"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aoxpVWAV_md4","executionInfo":{"elapsed":135,"status":"ok","timestamp":1638547629864,"user":{"displayName":"billh17","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12625841932899930070"},"user_tz":360},"outputId":"10f17df8-c2a5-425c-a63b-a10d8802264a"},"source":["if True:\n","    my_working_directory = 'drive/MyDrive/rlcard-colab/'\n","    %cd {my_working_directory}"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/rlcard-colab\n"]}]},{"cell_type":"markdown","metadata":{"id":"-TS7Nd6ZL9y9"},"source":["\n","#Definitions\n","* Transition = namedtuple('Transition', ['state', 'action', 'reward', 'next_state', 'next_legal_actions', 'done'])\n","* Trajectory is a list of transitions made by an agent\n","\n","# Agent\n","\n","The Agent has these functions:\n","* step(state) -> action_id\n","* eval_step(state) -> action_id, probabilites\n","* feed(trajectory)\n","* predict(state) -> q_values\n","\n","If the agent is not trainable:\n","* feed(trajectory) does nothing\n","* predict(state) returns None\n","\n","# Environment\n","\n","The Environment holds information about:\n","* game\n","* agents\n","* state_shape\n","* action_shape\n","* timestep (incremented every time any agent takes a step)\n","\n","The Environment has these functions:\n","* set_agents(agents)\n","* run(is_training) -> Trajectories, Payoffs\n","\n","The Environment determines:\n","* how the state of the game is represented\n","* what is the state_shape\n","* what is the action_shape\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YGfjPPfZu4HZ"},"source":["# Install RLCard"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EhtEtQjiRqfI","executionInfo":{"elapsed":3997,"status":"ok","timestamp":1638547634052,"user":{"displayName":"billh17","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12625841932899930070"},"user_tz":360},"outputId":"b49873c7-02bb-4d36-80dc-76fbd9540a20"},"source":["pip install rlcard"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: rlcard in /usr/local/lib/python3.7/dist-packages (1.0.5)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from rlcard) (1.1.0)\n","Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.7/dist-packages (from rlcard) (1.19.5)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9rSz-FE8SBkd","executionInfo":{"elapsed":3785,"status":"ok","timestamp":1638547637832,"user":{"displayName":"billh17","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12625841932899930070"},"user_tz":360},"outputId":"53be32f6-644c-4def-ce99-2ba6560efb3d"},"source":["pip install rlcard[torch]"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: rlcard[torch] in /usr/local/lib/python3.7/dist-packages (1.0.5)\n","Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.7/dist-packages (from rlcard[torch]) (1.19.5)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from rlcard[torch]) (1.1.0)\n","Requirement already satisfied: GitPython in /usr/local/lib/python3.7/dist-packages (from rlcard[torch]) (3.1.24)\n","Requirement already satisfied: gitdb2 in /usr/local/lib/python3.7/dist-packages (from rlcard[torch]) (4.0.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from rlcard[torch]) (1.10.0+cu111)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from rlcard[torch]) (3.2.2)\n","Requirement already satisfied: gitdb>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb2->rlcard[torch]) (4.0.9)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb>=4.0.1->gitdb2->rlcard[torch]) (5.0.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython->rlcard[torch]) (3.10.0.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->rlcard[torch]) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->rlcard[torch]) (3.0.6)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->rlcard[torch]) (1.3.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->rlcard[torch]) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->rlcard[torch]) (1.15.0)\n"]}]},{"cell_type":"markdown","metadata":{"id":"uJO13ODqT2F9"},"source":["# Imports"]},{"cell_type":"code","metadata":{"id":"zkpz33iLSfye"},"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import os\n","from copy import deepcopy\n","\n","from datetime import datetime\n","from pytz import timezone"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KFuHUQIRSuou"},"source":["import rlcard\n","from rlcard.agents.random_agent import RandomAgent\n","from rlcard.models.gin_rummy_rule_models import GinRummyNoviceRuleAgent\n","\n","from rlcard.agents.dqn_agent import DQNAgent\n","from rlcard.agents.dqn_agent import Estimator\n","from rlcard.agents.dqn_agent import EstimatorNetwork\n","from rlcard.agents.dqn_agent import Memory\n","\n","from rlcard.utils import reorganize\n","from rlcard.utils import get_device\n","from rlcard.utils import set_seed\n","from rlcard.utils import Logger\n","from rlcard.utils import tournament\n","from rlcard.utils import plot_curve\n","\n","from rlcard.envs.env import Env\n","from rlcard.envs.gin_rummy import GinRummyEnv\n","\n","from rlcard.games.gin_rummy.game import GinRummyGame\n","from rlcard.games.gin_rummy.player import GinRummyPlayer\n","from rlcard.games.gin_rummy.utils.move import ScoreNorthMove, ScoreSouthMove, DrawCardMove, PickupDiscardMove, KnockMove, GinMove, DeclareDeadHandMove\n","from rlcard.games.gin_rummy.utils.action_event import KnockAction, GinAction"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BFmB6BAyW5ld"},"source":["# Gin Rummy Scoring function\n","\n","The get_gin_rummy_environment function uses this to set the payoffs for the Gin Rummy game.\n","\n","A player gets 1 point for going out (knock or gin) else 0 points (dead hand or lose).\n","\n","The DQNAgent needs to learn how to form and keep melds so that he can go out.\n","\n","The DQNAgent should pickup the discard when it would form a meld.\n","Otherwise, he should draw a card (altough an exper player might break this rule sometimes)."]},{"cell_type":"code","metadata":{"id":"5_ev9aQfW895"},"source":["def get_payoff_gin_rummy(player: GinRummyPlayer, game: GinRummyGame) -> float:\n","    ''' Get the payoff of player:\n","            a) 1.0 if player gins\n","            b) 1.0 if player knocks\n","            c) 0.0 otherwise\n","        The goal is to have the agent learn how to knock and gin.\n","    Returns:\n","        payoff (int or float): payoff for player (higher is better)\n","    '''\n","    going_out_action = game.round.going_out_action\n","    going_out_player_id = game.round.going_out_player_id\n","    if going_out_player_id == player.player_id and isinstance(going_out_action, KnockAction):\n","        payoff = 1\n","    elif going_out_player_id == player.player_id and isinstance(going_out_action, GinAction):\n","        payoff = 1\n","    else:\n","        payoff = 0\n","    return payoff"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b5kn7UeXD0MB"},"source":["# Get Gin Rummy Environment function\n","\n","Note that we patch the Gin Rummy game that is in the environment."]},{"cell_type":"code","metadata":{"id":"2aXfm3Mu4Gww"},"source":["def get_gin_rummy_environment(seed) -> GinRummyEnv:\n","    # Make the environment with seed\n","    gin_rummy_env = rlcard.make('gin-rummy', config={'seed': seed})\n","\n","    # patch gin rummy game\n","    gin_rummy_env.game.judge.scorer.get_payoff = get_payoff_gin_rummy # wch: Note this\n","    gin_rummy_env.game.settings.is_always_knock = True # wch: Note this\n","\n","    # print info\n","    state_shape = gin_rummy_env.state_shape[0] # state_shape for player_id = 0\n","    action_shape = gin_rummy_env.action_shape\n","    num_actions = gin_rummy_env.game.get_num_actions()\n","    print(f'state_shape={state_shape}')\n","    print(f'action_shape={action_shape}')\n","    print(f'num_actions={num_actions}')\n","    return gin_rummy_env"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LXV59AqeUeos"},"source":["# Get DQNAgent function\n","\n","The train_dqn_agent function uses this to get the DQNAgent.\n","\n","The critical hyper parameters for the DQNAgent are:\n","\n","* replay_memory_size = 200000 rather than 20K\n","* replay_memory_init_size = 1000 rather than 100\n","* batch_size  =128 rather than 32\n","* train_every = 100 rather than 1\n","* mlp_layers = [128, 128, 128] rather than [64, 64]\n","* learning_rate=0.00005 You might want to modify this.\n","\n","The DQNAgent has instance variables and three components:\n","\n","* Instance variables:\n","    1. update_target_estimator_every\n","    2. train_every\n","    3. discount_factor\n","    4. epsilon_start\n","    5. epsilon_end\n","    6. epsilon_decay_step\n","\n","* Estimator:\n","    1. learning_rate\n","    2. device\n","\n","* Estimator Network:\n","    1. state_shape\n","    2. mlp_layers\n","    3. num_actions\n","\n","* Replay Memory:\n","    1. replay_memory_size\n","    2. replay_memory_init_size\n","    3. batch_size\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"0UEhG5_4UWTz"},"source":["def get_dqn_agent(env, model_name: str, log_dir: str):\n","    replay_memory_size = 200000 # 20000\n","    replay_memory_init_size = 1000 #100\n","    update_target_estimator_every = 1000\n","    discount_factor=0.99\n","    epsilon_start=1.0\n","    epsilon_end=0.1\n","    epsilon_decay_steps=20000\n","    batch_size=128 #32\n","    train_every=100 #1\n","    mlp_layers = [128, 128, 128] # [64, 64]\n","    learning_rate=0.00005 #0.00005\n","    device = get_device()\n","\n","    agent_path = os.path.join(log_dir, f'{model_name}.pth')\n","    if os.path.exists(agent_path):\n","        train_id = 2 # increment for each train iteration\n","        dqn_agent = torch.load(agent_path, map_location=device)\n","        dqn_agent.set_device(device)\n","        #dqn_agent.train_every = 100  # wch: note this\n","        #dqn_agent.batch_size = 128  # wch: note this\n","        #dqn_agent.memory.batch_size = dqn_agent.batch_size  # wch: note this\n","        #dqn_agent.replay_memory_init_size = 1000  # wch: note this\n","        #dqn_agent.q_estimator.optimizer.param_groups[0]['lr'] = 0.000005\n","        #dqn_agent.target_estimator.optimizer.param_groups[0]['lr'] = 0.000005\n","        print(f'train_id={train_id}; agent={dqn_agent}')\n","        print(f'train_steps={dqn_agent.train_t} time_steps={dqn_agent.total_t}')\n","    else:\n","        train_id = 1 # reset for first train iteration\n","        num_actions = env.game.get_num_actions()\n","        state_shape = env.state_shape[0] # state_shape for player_id = 0\n","        dqn_agent = DQNAgent(\n","            replay_memory_size,\n","            replay_memory_init_size,\n","            update_target_estimator_every,\n","            discount_factor,\n","            epsilon_start,\n","            epsilon_end,\n","            epsilon_decay_steps,\n","            batch_size,\n","            num_actions,\n","            state_shape,\n","            train_every,\n","            mlp_layers,\n","            learning_rate,\n","            device=device)\n","    print(f'log_dir={log_dir}')\n","    print(f'model_name={model_name}')\n","    print(dqn_agent.q_estimator.qnet)\n","    return dqn_agent"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i0jD0G12XQSw"},"source":["# Game Statistics class\n","\n","The Train class train function uses this to accumulate statistics for each completed game.\n","\n","The Train class train function uses this to show the accumulated statistics when the training session ends.\n"]},{"cell_type":"code","metadata":{"id":"VJjpfs_vXSyK"},"source":["class GameStatistics(object):  # interface\n","    def update_statistics(self, game):\n","        pass\n","\n","    def show_statistics(self):\n","        pass\n","\n","class GinRummyGameStatistics(GameStatistics):\n","    def __init__(self):\n","        super().__init__()\n","        self.game_count = 0\n","        self.north_draw_card_move_count = 0\n","        self.north_pickup_discard_move_count = 0\n","        self.knock_count = 0\n","        self.gin_count = 0\n","        self.declare_dead_hand_count = 0\n","        self.moves_per_game = 0\n","\n","    def update_statistics(self, game):\n","        move_sheet = game.round.move_sheet\n","        self.game_count += 1\n","        if len(move_sheet) > 2:\n","            north_draw_card_moves = [move for move in move_sheet if isinstance(move, DrawCardMove) and move.player.player_id == 0]\n","            self.north_draw_card_move_count += len(north_draw_card_moves)\n","            north_pickup_discard_moves = [move for move in move_sheet if isinstance(move, PickupDiscardMove) and move.player.player_id == 0]\n","            self.north_pickup_discard_move_count += len(north_pickup_discard_moves)\n","            going_out_move = move_sheet[-3]\n","            if isinstance(going_out_move, KnockMove):\n","                if going_out_move.player.player_id == 0:\n","                    self.knock_count += 1\n","            elif isinstance(going_out_move, GinMove):\n","                if going_out_move.player.player_id == 0:\n","                    self.gin_count += 1\n","            elif isinstance(going_out_move, DeclareDeadHandMove):\n","                self.declare_dead_hand_count += 1\n","        self.moves_per_game += len(move_sheet)\n","\n","    def show_statistics(self):\n","        print(f'game_count={self.game_count}')\n","        print(f'north_draw_card_moves_per_game={self.north_draw_card_move_count / self.game_count}')\n","        print(f'north_pickup_discard_moves_per_game={self.north_pickup_discard_move_count / self.game_count}')\n","        print(f'knock_count={self.knock_count}')\n","        print(f'gin_count={self.gin_count}')\n","        print(f'declare_dead_hand_count={self.declare_dead_hand_count}')\n","        print(f'average_moves_per_game={self.moves_per_game / self.game_count}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"znkxS0HYU2-8"},"source":["# Trainer class\n","\n","This class is initialized with an environment.\n","\n","You must set the agents for the environment before you call the train method.\n","\n","The first agent must be the agent to be trained.\n","\n","=====================================\n","\n","In this notebook, the first agent is the DQNAgent and the second agent is the GinRummyNoviceRuleAgent.\n","\n","Training the DQNAgent against the RandomAgent is easy, but the trained DQNAgent learns a lot of bad habits. The play of the trained DQNAgent against the RandomAgent results in long games where both players frequently pick up the opponent's discard. The DQNAgent learns how to form melds (so that the game doesn't end as a dead hand), but waste times picking up the opponent's discard.\n","\n","====================================="]},{"cell_type":"code","metadata":{"id":"A4IH20pdMB-d"},"source":["class Trainer(object):\n","\n","    def __init__(self, log_dir: str, model_name: str, env: Env):\n","        self.log_dir = log_dir\n","        self.model_name = model_name\n","        self.algorithm = 'dqn'\n","        self.env = env\n","        self.game_statistics = GameStatistics()\n","    \n","    def train(self, multiplier: int, seed):\n","        with Logger(self.log_dir) as logger:\n","            num_episodes = 1000 * multiplier  # 5000\n","            evaluate_every = 20 * multiplier  # 100\n","            num_eval_games = 300 # 2000\n","            # Seed numpy, torch, random\n","            set_seed(seed)\n","            for episode in range(num_episodes):\n","                # Generate data from the environment\n","                trajectories, payoffs = self.env.run(is_training=True)\n","                # Reorganaize the data to be state, action, reward, next_state, done\n","                trajectories = reorganize(trajectories, payoffs)\n","\n","                # Feed transitions into agent memory\n","                self.feed(trajectories)\n","\n","                # Keep game statistics\n","                self.game_statistics.update_statistics(game=self.env.game)\n","\n","                # Evaluate the performance. Play with random agents.\n","                if episode % evaluate_every == 0:\n","                    self.evaluate_performance(episode=episode, evaluate_every=evaluate_every, num_eval_games=num_eval_games, logger=logger)\n","                            \n","                # Save model when number of episodes is very large ?\n","                if episode > 0 and episode % 100000 == 0:\n","                    self.save_model(agent=self.env.agents[0])\n","\n","        # Show game statistics\n","        self.game_statistics.show_statistics()\n","        # Plot the learning curve\n","        self.plot_learning_curve(logger=logger)\n","        # Save model\n","        self.save_model(agent=self.env.agents[0])\n","    \n","    def feed(self, trajectories):\n","        # Feed transitions into agent memory, and train the agent\n","        # Here, we assume that DQN always plays the first position\n","        # and the other players play randomly (if any)\n","        for ts in trajectories[0]:\n","            self.env.agents[0].feed(ts)\n","\n","    def evaluate_performance(self, episode: int, evaluate_every: int, num_eval_games: int, logger):\n","        # Evaluate the performance. Play with random agents.\n","        logger.log(f\"\\n----------------------------------------\")\n","        logger.log(f\"\\nEpisode: {episode}\")\n","        logger.log_performance(self.env.timestep, tournament(self.env, num_eval_games)[0])\n","    \n","    def plot_learning_curve(self, logger):\n","        # Plot the learning curve\n","        csv_path, fig_path = logger.csv_path, logger.fig_path\n","        plot_curve(csv_path, fig_path, self.algorithm)\n","\n","    def save_model(self, agent):\n","        # Save model\n","        # Note: Saving the agent is better than saving just the model.\n","        save_path = os.path.join(self.log_dir, f'{self.model_name}.pth')\n","        torch.save(agent, save_path)\n","        print('Model saved in', save_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7s3KSERaVfRi"},"source":["# Get current time function"]},{"cell_type":"code","metadata":{"id":"cgCb3-HsVk86"},"source":["def get_current_time():\n","    return datetime.now(timezone('America/Chicago')).strftime('%I:%M:%S %p')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gPu5nSF94UzQ"},"source":["# Constants"]},{"cell_type":"code","metadata":{"id":"invcWghmVBBs"},"source":["my_log_dir='experiments/results_gin_rummy_dqn/'\n","my_model_name='model_gin_rummy_dqnagent'\n","my_seed = None\n","my_multiplier = 500"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3a4RzZpK4c9A"},"source":["# Train DQNAgent"]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"BvVyEJekelZz","outputId":"e7d334e5-7ecb-47b2-d456-312bd55da808"},"source":["%%time\n","print(f\"Start: {get_current_time()}\")\n","if torch.cuda.is_available():\n","    !nvidia-smi\n","# define gin rummy environment\n","my_gin_rummy_env = get_gin_rummy_environment(seed=my_seed)\n","# define learning agent\n","my_learning_agent = get_dqn_agent(env=my_gin_rummy_env, model_name=my_model_name, log_dir=my_log_dir)\n","# define opponent agent\n","my_opponent_agent = GinRummyNoviceRuleAgent()\n","# set agents for environment\n","my_gin_rummy_env.set_agents([my_learning_agent, my_opponent_agent])\n","# train dqn_agent\n","my_trainer = Trainer(log_dir=my_log_dir, model_name=my_model_name, env=my_gin_rummy_env)\n","my_trainer.game_statistics = GinRummyGameStatistics()\n","my_trainer.train(multiplier=my_multiplier, seed=my_seed)\n","print(f\"End: {get_current_time()}\")"],"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Start: 10:07:22 AM\n","state_shape=[5, 52]\n","action_shape=[None, None]\n","num_actions=110\n","--> Running on the CPU\n","log_dir=experiments/results_gin_rummy_dqn/\n","model_name=model_gin_rummy_dqnagent\n","EstimatorNetwork(\n","  (fc_layers): Sequential(\n","    (0): Flatten(start_dim=1, end_dim=-1)\n","    (1): BatchNorm1d(260, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): Linear(in_features=260, out_features=128, bias=True)\n","    (3): Tanh()\n","    (4): Linear(in_features=128, out_features=128, bias=True)\n","    (5): Tanh()\n","    (6): Linear(in_features=128, out_features=128, bias=True)\n","    (7): Tanh()\n","    (8): Linear(in_features=128, out_features=110, bias=True)\n","  )\n",")\n","\n","----------------------------------------\n","\n","Episode: 0\n","\n","----------------------------------------\n","  timestep     |  55\n","  reward       |  0.03\n","----------------------------------------\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"]},{"output_type":"stream","name":"stdout","text":["INFO - Step 1000, rl-loss: 0.36836180090904236\n","INFO - Copied model parameters to target network.\n","INFO - Step 101000, rl-loss: 0.08030931651592255\n","INFO - Copied model parameters to target network.\n","INFO - Step 201000, rl-loss: 0.030089031904935837\n","INFO - Copied model parameters to target network.\n","INFO - Step 259500, rl-loss: 0.024272767826914787\n","----------------------------------------\n","\n","Episode: 10000\n","\n","----------------------------------------\n","  timestep     |  544043\n","  reward       |  0.03\n","----------------------------------------\n","INFO - Step 301000, rl-loss: 0.03186260908842087\n","INFO - Copied model parameters to target network.\n","INFO - Step 401000, rl-loss: 0.02609328180551529\n","INFO - Copied model parameters to target network.\n","INFO - Step 501000, rl-loss: 0.014850076287984848\n","INFO - Copied model parameters to target network.\n","INFO - Step 515300, rl-loss: 0.01892142929136753\n","----------------------------------------\n","\n","Episode: 20000\n","\n","----------------------------------------\n","  timestep     |  1080666\n","  reward       |  0.043333333333333335\n","----------------------------------------\n","INFO - Step 601000, rl-loss: 0.012979437597095966\n","INFO - Copied model parameters to target network.\n","INFO - Step 701000, rl-loss: 0.013591847382485867\n","INFO - Copied model parameters to target network.\n","INFO - Step 770900, rl-loss: 0.008782955817878246\n","----------------------------------------\n","\n","Episode: 30000\n","\n","----------------------------------------\n","  timestep     |  1616379\n","  reward       |  0.043333333333333335\n","----------------------------------------\n","INFO - Step 801000, rl-loss: 0.00976131297647953\n","INFO - Copied model parameters to target network.\n","INFO - Step 901000, rl-loss: 0.009399889037013054\n","INFO - Copied model parameters to target network.\n","INFO - Step 1001000, rl-loss: 0.00614932319149375\n","INFO - Copied model parameters to target network.\n","INFO - Step 1026600, rl-loss: 0.01076465006917715\n","----------------------------------------\n","\n","Episode: 40000\n","\n","----------------------------------------\n","  timestep     |  2152678\n","  reward       |  0.043333333333333335\n","----------------------------------------\n","INFO - Step 1101000, rl-loss: 0.004404324106872082\n","INFO - Copied model parameters to target network.\n","INFO - Step 1201000, rl-loss: 0.004413611721247435\n","INFO - Copied model parameters to target network.\n","INFO - Step 1283700, rl-loss: 0.004772301763296127\n","----------------------------------------\n","\n","Episode: 50000\n","\n","----------------------------------------\n","  timestep     |  2692180\n","  reward       |  0.02\n","----------------------------------------\n","INFO - Step 1301000, rl-loss: 0.012623706832528114\n","INFO - Copied model parameters to target network.\n","INFO - Step 1401000, rl-loss: 0.003063387703150511\n","INFO - Copied model parameters to target network.\n","INFO - Step 1501000, rl-loss: 0.0029089224990457296\n","INFO - Copied model parameters to target network.\n","INFO - Step 1538800, rl-loss: 0.008934661746025085\n","----------------------------------------\n","\n","Episode: 60000\n","\n","----------------------------------------\n","  timestep     |  3226748\n","  reward       |  0.04\n","----------------------------------------\n","INFO - Step 1601000, rl-loss: 0.002645063679665327\n","INFO - Copied model parameters to target network.\n","INFO - Step 1701000, rl-loss: 0.0020842801313847303\n","INFO - Copied model parameters to target network.\n","INFO - Step 1793500, rl-loss: 0.001674553146585822\n","----------------------------------------\n","\n","Episode: 70000\n","\n","----------------------------------------\n","  timestep     |  3761136\n","  reward       |  0.043333333333333335\n","----------------------------------------\n","INFO - Step 1801000, rl-loss: 0.0028481693007051945\n","INFO - Copied model parameters to target network.\n","INFO - Step 1901000, rl-loss: 0.002163109602406621\n","INFO - Copied model parameters to target network.\n","INFO - Step 2001000, rl-loss: 0.0017852395540103316\n","INFO - Copied model parameters to target network.\n","INFO - Step 2048500, rl-loss: 0.006967631168663502\n","----------------------------------------\n","\n","Episode: 80000\n","\n","----------------------------------------\n","  timestep     |  4296623\n","  reward       |  0.03333333333333333\n","----------------------------------------\n","INFO - Step 2101000, rl-loss: 0.0016092098085209727\n","INFO - Copied model parameters to target network.\n","INFO - Step 2201000, rl-loss: 0.0015490255318582058\n","INFO - Copied model parameters to target network.\n","INFO - Step 2301000, rl-loss: 0.0011167717166244984\n","INFO - Copied model parameters to target network.\n","INFO - Step 2303200, rl-loss: 0.0014176982222124934\n","----------------------------------------\n","\n","Episode: 90000\n","\n","----------------------------------------\n","  timestep     |  4830059\n","  reward       |  0.043333333333333335\n","----------------------------------------\n","INFO - Step 2401000, rl-loss: 0.0013520990032702684\n","INFO - Copied model parameters to target network.\n","INFO - Step 2501000, rl-loss: 0.006595293525606394\n","INFO - Copied model parameters to target network.\n","INFO - Step 2559200, rl-loss: 0.0008489590254612267\n","----------------------------------------\n","\n","Episode: 100000\n","\n","----------------------------------------\n","  timestep     |  5366357\n","  reward       |  0.06\n","----------------------------------------\n","Model saved in experiments/results_gin_rummy_dqn/model_gin_rummy_dqnagent.pth\n","INFO - Step 2601000, rl-loss: 0.006660622078925371\n","INFO - Copied model parameters to target network.\n","INFO - Step 2701000, rl-loss: 0.000978723051957786\n","INFO - Copied model parameters to target network.\n","INFO - Step 2801000, rl-loss: 0.001129254582338035\n","INFO - Copied model parameters to target network.\n","INFO - Step 2813700, rl-loss: 0.0006920799496583641\n","----------------------------------------\n","\n","Episode: 110000\n","\n","----------------------------------------\n","  timestep     |  5899357\n","  reward       |  0.056666666666666664\n","----------------------------------------\n","INFO - Step 2901000, rl-loss: 0.0007155111525207758\n","INFO - Copied model parameters to target network.\n","INFO - Step 3001000, rl-loss: 0.008258196525275707\n","INFO - Copied model parameters to target network.\n","INFO - Step 3069600, rl-loss: 0.0006770711042918265\n","----------------------------------------\n","\n","Episode: 120000\n","\n","----------------------------------------\n","  timestep     |  6435485\n","  reward       |  0.03\n","----------------------------------------\n","INFO - Step 3101000, rl-loss: 0.0008659036830067635\n","INFO - Copied model parameters to target network.\n","INFO - Step 3201000, rl-loss: 0.00076734006870538\n","INFO - Copied model parameters to target network.\n","INFO - Step 3301000, rl-loss: 0.005030158441513777\n","INFO - Copied model parameters to target network.\n","INFO - Step 3326700, rl-loss: 0.006007768213748932\n","----------------------------------------\n","\n","Episode: 130000\n","\n","----------------------------------------\n","  timestep     |  6974805\n","  reward       |  0.056666666666666664\n","----------------------------------------\n","INFO - Step 3401000, rl-loss: 0.0005824919790029526\n","INFO - Copied model parameters to target network.\n","INFO - Step 3440500, rl-loss: 0.00036958622513338923"]}]},{"cell_type":"code","metadata":{"id":"DkUxfWOJWXcb"},"source":["print(\"Done\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jFAKtyozOoBv"},"source":["# play_gin_rummy_tournament function"]},{"cell_type":"code","metadata":{"id":"laUFkGdfOlbD"},"source":["def play_gin_rummy_tournament_with_agents(north_agent, south_agent, num_games: int):\n","    tournament_env = rlcard.make('gin-rummy', config={'seed': None})\n","    tournament_env.game.judge.scorer.get_payoff = get_payoff_gin_rummy # wch: Note this\n","    tournament_env.game.settings.is_always_knock = True # wch: Note this\n","    tournament_env.set_agents([north_agent, south_agent])\n","    payoffs = tournament(tournament_env, num_games)\n","    print(f\"payoffs=[{payoffs[0]}, {payoffs[1]}]\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pe5NEyJ3OyRa"},"source":["def play_gin_rummy_tournament(log_dir, model_name, num_games):\n","    agent_path = os.path.join(log_dir, f'{model_name}.pth')\n","    device = get_device()\n","    gin_rummy_dqn_agent = torch.load(agent_path, map_location=device)\n","    gin_rummy_dqn_agent.set_device(device)\n","    print(f\"gin_rummy_dqn_agent={gin_rummy_dqn_agent}\")\n","    ginRummyNoviceRuleAgent = GinRummyNoviceRuleAgent()\n","    num_actions = gin_rummy_dqn_agent.num_actions\n","    random_agent = RandomAgent(num_actions=num_actions)\n","    num_games_preliminary = 100\n","    for tournament_id in range(5):\n","        print(f\"----- preliminary tournament {tournament_id + 1} {num_games_preliminary} games ------\")\n","        play_gin_rummy_tournament_with_agents(gin_rummy_dqn_agent, ginRummyNoviceRuleAgent, num_games = num_games_preliminary)\n","        play_gin_rummy_tournament_with_agents(gin_rummy_dqn_agent, random_agent, num_games = num_games_preliminary)\n","        play_gin_rummy_tournament_with_agents(ginRummyNoviceRuleAgent, random_agent, num_games = num_games_preliminary)\n","    print(f\"----- final tournament gin_rummy_dqn_agent vs ginRummyNoviceRuleAgent {num_games} games  ------\")\n","    play_gin_rummy_tournament_with_agents(gin_rummy_dqn_agent, ginRummyNoviceRuleAgent, num_games = num_games)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qCUWSh_BO6o-"},"source":["# Gin Rummy Tournaments\n","\n","There are 5 preliminary tournaments of 100 games per tournament:\n","\n","*   gin_rummy_dqn_agent vs ginRummyNoviceRuleAgent\n","*   gin_rummy_dqn_agent vs randomAgent\n","*   ginRummyNoviceRuleAgent vs randomAgent\n","\n","There is one final tournament of {num_games} games per tournament:\n","\n","*   gin_rummy_dqn_agent vs ginRummyNoviceRuleAgent"]},{"cell_type":"code","metadata":{"id":"JLIJD0OrPCOB"},"source":["play_gin_rummy_tournament(log_dir=my_log_dir, model_name=my_model_name, num_games=2000)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"izsFS7v3PN5o"},"source":["# Print local variables"]},{"cell_type":"code","metadata":{"id":"zLs2SJ1mPaCl"},"source":["for my_type in ['str', 'int', 'float', 'list', 'dict', 'NoneType']:\n","    print(f'---------------------------------')\n","    print(f'{my_type}')\n","    %whos {my_type}\n","    print()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NwG0d2BzgD35"},"source":["%whos str int list dict NoneType"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m-HUvpgWvI9z"},"source":[""],"execution_count":null,"outputs":[]}]}